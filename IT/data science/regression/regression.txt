assumptions for linear regression
  linearirity
  no endogeneity σxε = 0: ∀ x, ε
  normality and homoscedasticity ε ~ N(0,σ^2)
  no autocorrelation σεiεj = 0: ∀i ≠ j
  no multilinearity p xixj !≈ 1 ∀ i,j; i ≠ j

log-log model

Y `dependent, predicted`
x1, x2 ... xk `independent, predictors`
Y = F(x1, x2, ..., xk)

#simple linear regression model
y = β0 + β1x1 + ε `population formula`
y `dependent variable`
x1 `independent variable`
β1 `quantifies effect`
β0 `constant`
ε `error`

y-hat = b0 + b1x1
x1 `sample data for independent variable`

correlation vs regression

correlation 
  relationship
  move together
  single point

regression
  one variable affects the other
  cause and effect
  one way
  line

sum of squares total SST or TSS
  sum n i=1(yi - _y)^2

sum of squares regression SSR or ESS
  sum n i=1(yhati - _y)^2

sum of squares error SSE or RSS
 sum n i=1 ei^2

SST = SSR + SSE

OLS (Ordinary Least Squares) 

R-squared = how accurate is model, measures how well your model fits your data
R-squared = SSR / SST

#multiple regression modal
y = β0 + β1x1 + β2x2 + ... βkxk + ε `population formula`
yhat b0 + b1x1 + b2x2 + ... + bkxk `multiple regression equation`
yhat `inferred (odvozeno) value `
b0 `intercept (zachytit)`
b1 `independent variable`

F-test
H0 = β0 = β1 = ... = βk = 0
H1: at least one βi ≠ 0
lower F-statistics means less significant model

Standardization

overfitting
  `too much focus on particular, so it has "missed the point"` - too much accuracy
underfitting
  `the model has not coptured the underlying of the data` - low accuracy

vif `variance_inflation_factor`
  VIF = 1 `no multicollinearity`
  1 < VIF < 5 `perfectly ok`
  10 < VIF `unacceptable`

so = standatní otázka = co je, k čemu se používá, ukázka příkladu

?Co je simple linear regression, co ukazuje, ukázka na příkladu
?Co je multiple lirear regression...
?so, p-value
?so, STA
?so, GPA
?so, std / standard deviation
?so, log function
?so, Multicollinearity