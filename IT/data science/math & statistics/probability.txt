- Probability
  from 0 to 1
  P=target outcome / all outcomes
  complementary A``
  frequency

- Combinations
  permutation 
    combinations in set of items when all elements are in sample space 
    `all in`
    (1,2,3) (1,3,2) (2,3,1) (2,1,3) (3,2,1) (3,1,2)
    Pn = n!
    Pn = n * (n-1) * (n- 2) ... 1 = n! = fatorial
    3! = 1 * 2 * 3
    n! = (n+1)! / (n+1)
    repetitve
    non-repetitve
  variations
    total number of ways we can pick and arrange some elements of a given set
    `only some`
    7^4
    repetitve
    non-repetitve
      Vp^n = n! / (n-p)!
  combinations
    every permutation of a combination is a different variation
    `ignore order`
    Cp^n = Vp^n / Pp
    C4^10 = 10! / 4! 6! = 7 * 8 * 9 * 10 / 1 * 2* 3 *4 = 5040 / 24 = 210
    symetry
      Cp^n = C(n-p)^n
    combinations with separete sample spaces
      C = n1*n2*n3 ...
      C = 3*5*3
    repetitve
    non-repetitve

- Beyesian inference
  Event
    not touch
    intersection
      A âˆ© B znaÄÃ­ mnoÅ¾inu, kterÃ¡ obsahuje prvky, kterÃ© jsou mnoÅ¾inÃ¡m A a B spoleÄnÃ©.
    subset
      A âŠ† B #A is fully containet in set B
    unions
      A âˆª B znaÄÃ­ mnoÅ¾inu, kterÃ¡ obsahuje prvky, kterÃ© jsou alespoÅˆ v jednÃ© z mnoÅ¾in A a B.
    mutually exclusive
      two Events without intersection
    independent events
      P(A|B) znaÄÃ­ pravdÄ›podobnost jevu A za pÅ™edpokladu, Å¾e nastane jev B.
      P(A|B) = A / B
    additive law, 
      P(Union) = P(A) + P(B) - P(Intersection)
      P(Intersection) = P(A) + P(B) - P(Union)
    multiplition rule
      P(A âˆ© B) = P(A|B) * P(B)
    Bayes Rule
      relation between independent events
      P(A|B) = P(B|A)*P(A) / P(B)

  x âˆˆ A # x is part of set A 
  x âˆ‰ A # x is not part of set A
  âˆ€ x: P(x) znamenÃ¡, Å¾e P(x) platÃ­ pro vÅ¡echna x.
  âˆ€ x x âˆˆ A : x is even #for all x in A, such that, x is even
  https://cs.wikipedia.org/wiki/Matematick%C3%A9_symboly_a_zna%C4%8Dky

  
- distribution
  ğ‘ƒ(ğ‘Œ = ğ‘¦) is equivalent to p(ğ‘¦)
  Y = actual outcome
  E = expected outcome??
  y = one of the possible outcomes
  Î¼ = mean = the sum of the values divided by the number of values
  Ïƒ = varience
  Î» = lambda
  Î¼ = Ïƒ^2 = Î»
  ~ = ÃºmÄ›rnost = proportion
  k = degrees of freedom
  s = scale parameter

  CDF = Cumulative Distribution Function
  PDF = Probability density function

  population data is equivalent to "all the data"

  #distribution types    
    - Discrete
      finite number of outcomes
      ğ‘· ğ’€ â‰¤ ğ’š = ğ‘· ğ’€ < ğ’š + ğŸ
      Discrete Uniform Distribution
      Bernoulli Distribution
        ğ’€~ ğ‘©ğ’†ğ’“ğ’(ğ’‘)
        `true / false` one trial
        ğ‘¬ (ğ’€) = ğ’‘
        ğ‘½ğ’‚ğ’“(ğ’€) = ğ’‘ Ã— (ğŸ âˆ’ ğ’‘)
      Binomial Distribution
        A sequence of identical Bernoulli events 
        ğ’€~ ğ‘©(ğ’, ğ’‘)
        P(Y= y) = C(y,n)*p^y* (1-p)^n-y
      Poisson Distribution
        Measures the frequency over an interval of time or distance
        ğ’€~ ğ‘·ğ’(Î»)
        e = eulers number â‰ˆ 2.72
        P(Y) = Î»^y*e^-Î» / y!

    Uniform Distribution
      ğ’€~ ğ‘¼(ğ’‚, ğ’ƒ)
      `same probability for all, ex. rolling a single die`
    
    - Continuous
      Have infinitely many consecutive possible values
      `smooth`
      ğ‘· (ğ’€ < ğ’š) = ğ‘· (ğ’€ â‰¤ ğ’š)

      Standardizing a Normal Distribution
        mean is 0 => Î¼ = 0
        variance is 1 => Ïƒ = 1
        z = y - Î¼ / Ïƒ

      Normal Distribution
        most natural events follow
        bell-shaped curve
        ğ’€~ ğ‘µ(Î¼, Ïƒ^ğŸ)
        ğ‘¬ ğ’€ = Î¼
        68, 95, 99.7 law
          68% of all its values should fall in the interval: (Î¼ âˆ’ ğˆ, ğ + ğˆ)
      Studentsâ€™ T Distribution
        ğ’€~ ğ’• (ğ’Œ)
        A small sample size approximation of a Normal Distribution.
        `symmetric curve`
      Chi-Squared Distribution
        ğ’€~ ğŒğŸ(ğ’Œ)
        Its graph is asymmetric and skewed to the right
      Exponential Distribution
        usually observed in events which significantly change early on.
        Y~ Exp (Î»)
        E(Y) = 1 / Î» 
      Logistic Distribution
        Often used in sports to anticipate how a playerâ€™s or
        teamâ€™s performance can determine the outcome of
        the match.
        Y~ Logistic (Î¼, s)
        E(Y) = Î¼
        Var(Y) = s^2*Ï€^2 / 3
